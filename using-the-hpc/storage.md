# Storage

The HPC provides an array of data storage platforms, each designed with a particular purpose in mind. Storage areas are broadly divided into two categories: those intended for user data and those intended for project data. Within each of the two categories, we provide different sub-areas, each with an intended purpose:

Storage Area  | Path                                                 | Purpose
------------- | ---------------------------------------------------- | ------------------------------------------
User Home     | /home/$USER                                          | Long-term data for routine access
User Scratch  | /lustre/group/$USER                                  | Short-term project data for fast, batch-job access that is not shared
Project Share | /lustre/group/proj-shared/$USER                      | Short-term project data for fast, batch-job access that's shared with other project members
World Share   | /lustre/group/world-shared/$USER                     | Short-term project data for fast, batch-job access that is shared globally
Local Scratch | $localscratch                                        | Fast read/write access during a batch job
User Archive  | HPSS (if applicable)                                 | Placeholder


#### USER HOME

Home directories for each user are NFS-mounted on all HPC systems and are intended to store long-term, frequently-accessed user data. User Home areas are not backed up. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems.

#### USER SCRATCH

Project members get an individual User Scratch directory; these reside in the high-capacity Lustre® file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to HPSS Archive space as soon as the files are not actively being used. If a file system associated with your User Scratch directory is nearing capacity, HPC Support may contact you to request that you reduce the size of your Member scratch directory.

#### PROJECT SHARE

Individual Project Share directories reside in the high-capacity Lustre file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Because of the scratch nature of the file system, it is not backed up. If a file system associated with Project Share storage is nearing capacity, HPC Support may contact the PI of the project to request that he or she reduce the size of the Project scratch directory.

#### WORLD SHARE

Each project has a World Share directory that resides in the high-capacity Lustre file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Because of the scratch nature of the file system, it is not backed up. If a file system associated with World Share storage is nearing capacity, HPC Support may contact the PI of the project to request that he or she reduce the size of the World Work directory.

#### LOCAL SCRATCH STORAGE  

A fast solid state disk (SSD) area intended for parallel access to temporary storage in the form of scratch directories. This area is local to the computational node. This directory is, for example, intended to hold temporary and intermediate output generated by a user’s job. This is a run time only file system which is created at the start of a batch job and is purged at the end of the job. Files should not be retained in this file system and should be migrated to Lustre scratch or archival storage before finishing the job.

Path for local scratch storage is available during job runtime via environment variable `$localscratch`. Variable `$localscratch` typically has the form `/localscratch/tmp.$USER.$PBS_JOBID.or-condo-pbs01` and is specific to the user and to the scheduled job.

#### PROJECT STORAGE \(WARP\)  

A NFS area intended for temporary data storage for moving data off the Lustre file system. This area is local to the computational node. This directory is, for example, intended to hold temporary and intermediate output generated by a user’s job. This is a run time only file system which is created at the start of a batch job and is purged at the end of the job. Files should not be retained in this file system and should be migrated to Lustre scratch or archival storage before finishing the job.

## Data Retention, Purge, & Quotas

#### SUMMARY

The following table details quota, backup, purge, and retention information for each user-centric and project-centric storage area available on the HPC.

#### DATA STORAGE RESOURCES

Area      | Path  | Type   | Permissions     | Quota | Backups | Purged | Retention
--------- | ----- | ------ | --------------- | ----- | ------- | ------ | ---------
User Home     | /home/$USER                | NFS    | User-controlled | 10 GB | No      | No     | NA
User Scratch  | /lustre/group/$USER        | Lustre | 700             | TBD   | No      | No     | TBD Days
Project Share | /lustre/group/proj-share   | Lustre | 770             | TBD   | No      | No     | TBD Days
World Share   | /lustre/group/world-share  | Lustre | 775             | TBD   | No      | No     | TBD Days
Local Scratch | /lustre/group/scratch      | Lustre | 770             | TBD   | No      | No     | TBD Days
